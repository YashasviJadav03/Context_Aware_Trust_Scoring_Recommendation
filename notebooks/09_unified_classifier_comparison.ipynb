{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Fake vs Real Review Classifier - Complete Model Comparison\n",
                "\n",
                "This notebook implements and compares **three different classifiers**:\n",
                "1. **Logistic Regression** - Baseline\n",
                "2. **Linear SVM** - Text specialist\n",
                "3. **XGBoost** - Non-linear power\n",
                "\n",
                "All models share the same preprocessing pipeline for efficiency."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Import Required Libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.svm import LinearSVC\n",
                "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
                "from scipy.sparse import hstack\n",
                "\n",
                "# XGBoost (install with: pip install xgboost)\n",
                "try:\n",
                "    from xgboost import XGBClassifier\n",
                "    XGBOOST_AVAILABLE = True\n",
                "except ImportError:\n",
                "    print(\"WARNING:  XGBoost not installed. Run: pip install xgboost\")\n",
                "    XGBOOST_AVAILABLE = False\n",
                "from scipy.special import expit\n",
                "\n",
                "print(\" Libraries imported successfully\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load and Filter Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Loading dataset...\")\n",
                "df = pd.read_csv(\"../data/processed/labeled_reviews.csv\")\n",
                "\n",
                "print(f\"Original dataset shape: {df.shape}\")\n",
                "print(f\"\\nLabel distribution:\")\n",
                "print(df['fake_label'].value_counts())\n",
                "print(f\"\\nConfidence distribution:\")\n",
                "print(df['label_confidence'].value_counts())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.1 Filter to High Confidence Samples\n",
                "\n",
                "**Critical Improvement:** Keep only high-confidence labels to reduce noise."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Filtering to high confidence samples...\")\n",
                "\n",
                "# Keep only high_real and high_fake\n",
                "df_filtered = df[df['label_confidence'].isin(['high_real', 'high_fake'])].copy()\n",
                "\n",
                "print(f\"Filtered dataset size: {len(df_filtered)}\")\n",
                "print(f\"Removed: {len(df) - len(df_filtered)} samples ({(len(df) - len(df_filtered))/len(df)*100:.2f}%)\")\n",
                "print(f\"\\nFiltered label distribution:\")\n",
                "print(df_filtered['fake_label'].value_counts())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Shared Preprocessing Pipeline"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.1 Select Features"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Text Feature\n",
                "X_text = df_filtered[\"review_text\"].astype(str)\n",
                "\n",
                "# Numerical Features\n",
                "X_numeric = df_filtered[[\"review_length\", \"rating_deviation\"]]\n",
                "\n",
                "# Target\n",
                "y = df_filtered[\"fake_label\"]\n",
                "\n",
                "print(f\"Text features: {len(X_text)} samples\")\n",
                "print(f\"Numeric features: {X_numeric.shape}\")\n",
                "print(f\"Target distribution:\\n{y.value_counts()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.2 Train-Test Split"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X_text_train, X_text_test, X_num_train, X_num_test, y_train, y_test = train_test_split(\n",
                "    X_text, X_numeric, y,\n",
                "    test_size=0.2,\n",
                "    random_state=42,\n",
                "    stratify=y\n",
                ")\n",
                "\n",
                "print(f\"Training samples: {len(y_train)}\")\n",
                "print(f\"Test samples: {len(y_test)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.3 TF-IDF Vectorization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "tfidf = TfidfVectorizer(\n",
                "    stop_words=\"english\",\n",
                "    max_features=5000,\n",
                "    ngram_range=(1, 2)\n",
                ")\n",
                "\n",
                "X_tfidf_train = tfidf.fit_transform(X_text_train)\n",
                "X_tfidf_test = tfidf.transform(X_text_test)\n",
                "\n",
                "print(f\"TF-IDF train shape: {X_tfidf_train.shape}\")\n",
                "print(f\"TF-IDF test shape: {X_tfidf_test.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.4 Scale Numeric Features"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "scaler = StandardScaler()\n",
                "X_num_train_scaled = scaler.fit_transform(X_num_train)\n",
                "X_num_test_scaled = scaler.transform(X_num_test)\n",
                "\n",
                "print(\" Numeric features scaled\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.5 Combine Features"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X_train = hstack([X_tfidf_train, X_num_train_scaled])\n",
                "X_test = hstack([X_tfidf_test, X_num_test_scaled])\n",
                "\n",
                "print(f\"Combined train shape: {X_train.shape}\")\n",
                "print(f\"Combined test shape: {X_test.shape}\")\n",
                "print(\"\\n Preprocessing complete - ready to train models!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Model 1: Logistic Regression (Baseline)\n",
                "\n",
                "**Why:** Simple, interpretable baseline for binary classification."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Training Logistic Regression...\")\n",
                "\n",
                "lr_model = LogisticRegression(\n",
                "    max_iter=2000,\n",
                "    class_weight=\"balanced\",\n",
                "    random_state=42\n",
                ")\n",
                "\n",
                "lr_model.fit(X_train, y_train)\n",
                "\n",
                "# Use threshold 0.65 for more conservative predictions\n",
                "lr_pred_proba = lr_model.predict_proba(X_test)[:, 1]\n",
                "lr_pred = (lr_pred_proba >= 0.65).astype(int)\n",
                "\n",
                "# Calculate metrics\n",
                "lr_accuracy = accuracy_score(y_test, lr_pred)\n",
                "lr_precision = precision_score(y_test, lr_pred)\n",
                "lr_recall = recall_score(y_test, lr_pred)\n",
                "lr_f1 = f1_score(y_test, lr_pred)\n",
                "\n",
                "print(\"\\n Logistic Regression Results (threshold=0.65):\")\n",
                "print(f\"   Accuracy:  {lr_accuracy:.4f}\")\n",
                "print(f\"   Precision: {lr_precision:.4f}\")\n",
                "print(f\"   Recall:    {lr_recall:.4f}\")\n",
                "print(f\"   F1-Score:  {lr_f1:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Model 2: Linear SVM (Text Specialist)\n",
                "\n",
                "**Why:** Designed for high-dimensional sparse data (TF-IDF)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Training Linear SVM...\")\n",
                "\n",
                "svm_model = LinearSVC(\n",
                "    class_weight=\"balanced\",\n",
                "    random_state=42,\n",
                "    max_iter=2000,\n",
                "    dual=False\n",
                ")\n",
                "\n",
                "svm_model.fit(X_train, y_train)\n",
                "\n",
                "# LinearSVC doesn't have predict_proba, use decision_function\n",
                "# Convert decision scores to probabilities using sigmoid\n",
                "from scipy.special import expit\n",
                "svm_decision = svm_model.decision_function(X_test)\n",
                "svm_pred_proba = expit(svm_decision)  # Sigmoid to get probabilities\n",
                "svm_pred = (svm_pred_proba >= 0.65).astype(int)\n",
                "\n",
                "# Calculate metrics\n",
                "svm_accuracy = accuracy_score(y_test, svm_pred)\n",
                "svm_precision = precision_score(y_test, svm_pred)\n",
                "svm_recall = recall_score(y_test, svm_pred)\n",
                "svm_f1 = f1_score(y_test, svm_pred)\n",
                "\n",
                "print(\"\\n Linear SVM Results (threshold=0.65):\")\n",
                "print(f\"   Accuracy:  {svm_accuracy:.4f}\")\n",
                "print(f\"   Precision: {svm_precision:.4f}\")\n",
                "print(f\"   Recall:    {svm_recall:.4f}\")\n",
                "print(f\"   F1-Score:  {svm_f1:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Model 3: XGBoost (Non-Linear Power)\n",
                "\n",
                "**Why:** Captures non-linear feature interactions.\n",
                "\n",
                "**Note:** Requires `pip install xgboost`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if XGBOOST_AVAILABLE:\n",
                "    print(\"Training XGBoost...\")\n",
                "    \n",
                "    # Calculate scale_pos_weight\n",
                "    scale_pos_weight = len(y_train[y_train==0]) / len(y_train[y_train==1])\n",
                "    \n",
                "    xgb_model = XGBClassifier(\n",
                "        n_estimators=200,\n",
                "        max_depth=6,\n",
                "        learning_rate=0.1,\n",
                "        scale_pos_weight=scale_pos_weight,\n",
                "        eval_metric=\"logloss\",\n",
                "        random_state=42,\n",
                "        tree_method='hist',\n",
                "        verbosity=0\n",
                "    )\n",
                "    \n",
                "    xgb_model.fit(X_train, y_train)\n",
                "    \n",
                "    # Use threshold 0.65 for more conservative predictions\n",
                "    xgb_pred_proba = xgb_model.predict_proba(X_test)[:, 1]\n",
                "    xgb_pred = (xgb_pred_proba >= 0.65).astype(int)\n",
                "    \n",
                "    # Calculate metrics\n",
                "    xgb_accuracy = accuracy_score(y_test, xgb_pred)\n",
                "    xgb_precision = precision_score(y_test, xgb_pred)\n",
                "    xgb_recall = recall_score(y_test, xgb_pred)\n",
                "    xgb_f1 = f1_score(y_test, xgb_pred)\n",
                "    \n",
                "    print(\"\\n XGBoost Results (threshold=0.65):\")\n",
                "    print(f\"   Accuracy:  {xgb_accuracy:.4f}\")\n",
                "    print(f\"   Precision: {xgb_precision:.4f}\")\n",
                "    print(f\"   Recall:    {xgb_recall:.4f}\")\n",
                "    print(f\"   F1-Score:  {xgb_f1:.4f}\")\n",
                "else:\n",
                "    print(\"WARNING:  XGBoost not available. Install with: pip install xgboost\")\n",
                "    xgb_accuracy = xgb_precision = xgb_recall = xgb_f1 = None"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Model Comparison\n",
                "\n",
                "Side-by-side comparison of all three models."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\" * 70)\n",
                "print(\"MODEL COMPARISON SUMMARY\")\n",
                "print(\"=\" * 70)\n",
                "\n",
                "# Create comparison dataframe\n",
                "comparison_data = {\n",
                "    'Model': ['Logistic Regression', 'Linear SVM'],\n",
                "    'Accuracy': [lr_accuracy, svm_accuracy],\n",
                "    'Precision': [lr_precision, svm_precision],\n",
                "    'Recall': [lr_recall, svm_recall],\n",
                "    'F1-Score': [lr_f1, svm_f1]\n",
                "}\n",
                "\n",
                "if XGBOOST_AVAILABLE and xgb_f1 is not None:\n",
                "    comparison_data['Model'].append('XGBoost')\n",
                "    comparison_data['Accuracy'].append(xgb_accuracy)\n",
                "    comparison_data['Precision'].append(xgb_precision)\n",
                "    comparison_data['Recall'].append(xgb_recall)\n",
                "    comparison_data['F1-Score'].append(xgb_f1)\n",
                "\n",
                "comparison_df = pd.DataFrame(comparison_data)\n",
                "print(\"\\n\", comparison_df.to_string(index=False))\n",
                "\n",
                "# Find best model for each metric\n",
                "print(\"\\n\" + \"=\" * 70)\n",
                "print(\"BEST MODELS PER METRIC\")\n",
                "print(\"=\" * 70)\n",
                "for metric in ['Accuracy', 'Precision', 'Recall', 'F1-Score']:\n",
                "    best_idx = comparison_df[metric].idxmax()\n",
                "    best_model = comparison_df.loc[best_idx, 'Model']\n",
                "    best_value = comparison_df.loc[best_idx, metric]\n",
                "    print(f\"{metric:12} → {best_model:20} ({best_value:.4f})\")\n",
                "\n",
                "print(\"\\n\" + \"=\" * 70)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 7.1 Percentage Improvements"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"IMPROVEMENTS OVER LOGISTIC REGRESSION BASELINE\")\n",
                "print(\"=\" * 70)\n",
                "\n",
                "# SVM vs LR\n",
                "svm_acc_imp = ((svm_accuracy - lr_accuracy) / lr_accuracy) * 100\n",
                "svm_prec_imp = ((svm_precision - lr_precision) / lr_precision) * 100\n",
                "svm_rec_imp = ((svm_recall - lr_recall) / lr_recall) * 100\n",
                "svm_f1_imp = ((svm_f1 - lr_f1) / lr_f1) * 100\n",
                "\n",
                "print(\"\\n Linear SVM vs Logistic Regression:\")\n",
                "print(f\"   Accuracy:  {svm_acc_imp:+.2f}%\")\n",
                "print(f\"   Precision: {svm_prec_imp:+.2f}%\")\n",
                "print(f\"   Recall:    {svm_rec_imp:+.2f}%\")\n",
                "print(f\"   F1-Score:  {svm_f1_imp:+.2f}%\")\n",
                "\n",
                "if XGBOOST_AVAILABLE and xgb_f1 is not None:\n",
                "    xgb_acc_imp = ((xgb_accuracy - lr_accuracy) / lr_accuracy) * 100\n",
                "    xgb_prec_imp = ((xgb_precision - lr_precision) / lr_precision) * 100\n",
                "    xgb_rec_imp = ((xgb_recall - lr_recall) / lr_recall) * 100\n",
                "    xgb_f1_imp = ((xgb_f1 - lr_f1) / lr_f1) * 100\n",
                "    \n",
                "    print(\"\\n XGBoost vs Logistic Regression:\")\n",
                "    print(f\"   Accuracy:  {xgb_acc_imp:+.2f}%\")\n",
                "    print(f\"   Precision: {xgb_prec_imp:+.2f}%\")\n",
                "    print(f\"   Recall:    {xgb_rec_imp:+.2f}%\")\n",
                "    print(f\"   F1-Score:  {xgb_f1_imp:+.2f}%\")\n",
                "\n",
                "print(\"\\n\" + \"=\" * 70)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 7.2 Confusion Matrices"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"CONFUSION MATRICES\")\n",
                "print(\"=\" * 70)\n",
                "\n",
                "print(\"\\n Logistic Regression:\")\n",
                "lr_cm = confusion_matrix(y_test, lr_pred)\n",
                "print(lr_cm)\n",
                "print(\"[[TN  FP]\\n [FN  TP]]\")\n",
                "\n",
                "print(\"\\n Linear SVM:\")\n",
                "svm_cm = confusion_matrix(y_test, svm_pred)\n",
                "print(svm_cm)\n",
                "print(\"[[TN  FP]\\n [FN  TP]]\")\n",
                "\n",
                "if XGBOOST_AVAILABLE and xgb_f1 is not None:\n",
                "    print(\"\\n XGBoost:\")\n",
                "    xgb_cm = confusion_matrix(y_test, xgb_pred)\n",
                "    print(xgb_cm)\n",
                "    print(\"[[TN  FP]\\n [FN  TP]]\")\n",
                "\n",
                "print(\"\\n\" + \"=\" * 70)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Recommendations\n",
                "\n",
                "Based on the results, here's when to use each model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"MODEL SELECTION GUIDE\")\n",
                "print(\"=\" * 70)\n",
                "\n",
                "print(\"\\n Use Logistic Regression when:\")\n",
                "print(\"   • You need a quick baseline\")\n",
                "print(\"   • Interpretability is critical\")\n",
                "print(\"   • Fast training/prediction is required\")\n",
                "\n",
                "print(\"\\n Use Linear SVM when:\")\n",
                "print(\"   • You have high-dimensional text data\")\n",
                "print(\"   • You want better performance than LR\")\n",
                "print(\"   • Interpretability is still important\")\n",
                "\n",
                "if XGBOOST_AVAILABLE:\n",
                "    print(\"\\n Use XGBoost when:\")\n",
                "    print(\"   • You need maximum performance\")\n",
                "    print(\"   • You can afford longer training time\")\n",
                "    print(\"   • You want feature importance insights\")\n",
                "    print(\"   • You have high-confidence labels\")\n",
                "\n",
                "# Determine best overall model\n",
                "best_f1_idx = comparison_df['F1-Score'].idxmax()\n",
                "best_model = comparison_df.loc[best_f1_idx, 'Model']\n",
                "best_f1_value = comparison_df.loc[best_f1_idx, 'F1-Score']\n",
                "\n",
                "print(\"\\n\" + \"=\" * 70)\n",
                "print(f\" RECOMMENDED MODEL: {best_model}\")\n",
                "print(f\"   Best F1-Score: {best_f1_value:.4f}\")\n",
                "print(\"=\" * 70)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Summary\n",
                "\n",
                "**Key Takeaways:**\n",
                "- All models trained on high-confidence labels only\n",
                "- Shared preprocessing pipeline for efficiency\n",
                "- Progressive improvement from LR → SVM → XGBoost\n",
                "- Choose model based on your priorities (speed vs performance vs interpretability)\n",
                "\n",
                "**Next Steps:**\n",
                "- Save the best model for deployment\n",
                "- Perform hyperparameter tuning\n",
                "- Try ensemble methods"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}