{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "03d4baef",
            "metadata": {},
            "source": [
                "# Web Scraping: Flipkart Reviews\n",
                "\n",
                "**Objective:** Scrape 100-200 reviews (Review Text, Rating, Date) for a single product from Flipkart.\n",
                "**Output:** Save to `../data/raw/scraped_reviews.csv`.\n",
                "\n",
                "> **Note:** This scraped data is used **only for validation**, not for model training."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "431c3395",
            "metadata": {},
            "outputs": [],
            "source": [
                "import requests\n",
                "from bs4 import BeautifulSoup\n",
                "import pandas as pd\n",
                "import time\n",
                "import random\n",
                "from pathlib import Path\n",
                "import os\n",
                "\n",
                "# Setup paths\n",
                "# Robustly find project root\n",
                "current_dir = Path(os.getcwd())\n",
                "if current_dir.name == \"notebooks\":\n",
                "    PROJECT_ROOT = current_dir.parent\n",
                "elif (current_dir / \"notebooks\").exists():\n",
                "    PROJECT_ROOT = current_dir\n",
                "else:\n",
                "    # Fallback or assume we are deeper/somewhere else\n",
                "    PROJECT_ROOT = Path(\"..\").resolve()\n",
                "\n",
                "DATA_DIR = PROJECT_ROOT / \"data\" / \"raw\"\n",
                "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
                "OUTPUT_FILE = DATA_DIR / \"scraped_reviews.csv\"\n",
                "\n",
                "print(f\"Project Root: {PROJECT_ROOT}\")\n",
                "print(f\"Data Directory: {DATA_DIR}\")\n",
                "print(f\"Output File: {OUTPUT_FILE}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3f41bada",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Target URL (Flipkart Product Reviews Page)\n",
                "BASE_URL = \"https://www.flipkart.com/tibra-collection-men-kurta-pyjama-set/product-reviews/itm82cbf59f5a028?pid=ETHFXSKHQPGZESBN&lid=LSTETHFXSKHQPGZESBNJOPCLT&marketplace=FLIPKART\"\n",
                "\n",
                "# Headers to mimic a real browser\n",
                "HEADERS = {\n",
                "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
                "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
                "    \"Accept-Encoding\": \"gzip, deflate, br\",\n",
                "    \"Connection\": \"keep-alive\"\n",
                "}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a3b97acb",
            "metadata": {},
            "outputs": [],
            "source": [
                "def scrape_flipkart_reviews(base_url, target_count=150):\n",
                "    reviews_data = []\n",
                "    page = 1\n",
                "    \n",
                "    while len(reviews_data) < target_count:\n",
                "        url = f\"{base_url}&page={page}\"\n",
                "        print(f\"Scraping page {page}: {url}...\")\n",
                "        \n",
                "        try:\n",
                "            response = requests.get(url, headers=HEADERS)\n",
                "            if response.status_code != 200:\n",
                "                print(f\"Failed to retrieve page {page}. Status code: {response.status_code}\")\n",
                "                break\n",
                "            \n",
                "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
                "            \n",
                "            # Flipkart review container class (subject to change, checking common ones)\n",
                "            # As of late 2024/2025, these classes are common but might need adjustment if Flipkart updates UI\n",
                "            review_cards = soup.find_all(\"div\", class_=\"cPHDOP col-12-12\")\n",
                "            \n",
                "            # Filter for actual review cards (sometimes other elements share the column class)\n",
                "            # usually specific reviews correspond to these internal structures\n",
                "            \n",
                "            # Fallback/Alternate search if main container isn't clear, lets look for rating blocks\n",
                "            # Common rating class: _3LWZlK\n",
                "            \n",
                "            # Let's iterate over a generic \"row\" or review block if specific container fails\n",
                "            # A more robust selection often involves looking for specific child elements\n",
                "            \n",
                "            found_on_page = 0\n",
                "            \n",
                "            # More precise selector for review blocks\n",
                "            blocks = soup.find_all(\"div\", class_=\"_27M-vq\") # Common wrapper for review\n",
                "            if not blocks:\n",
                "                 blocks = soup.find_all(\"div\", class_=\"col _2wzgFH K0kLPL\") # Older but still seen\n",
                "\n",
                "            if not blocks:\n",
                "               # Fallback to finding by components if container class changed\n",
                "               # This is risky but lets try to grab all text blocks\n",
                "               print(\"Warning: Standard review containers not found. Output might be empty. Check CSS selectors.\")\n",
                "            \n",
                "            for card in blocks:\n",
                "                try:\n",
                "                    # Rating\n",
                "                    rating_tag = card.find(\"div\", class_=\"_3LWZlK\")\n",
                "                    if not rating_tag:\n",
                "                        # sometimes ratings < 3 have different color/class, but usually _3LWZlK is consistent for structure or it might be _1BLPMq\n",
                "                        pass\n",
                "                    rating = rating_tag.text.strip() if rating_tag else None\n",
                "                    \n",
                "                    # Text\n",
                "                    text_tag = card.find(\"div\", class_=\"t-ZTKy\")\n",
                "                    if text_tag:\n",
                "                        # remove 'READ MORE' if present\n",
                "                        review_text = text_tag.get_text(separator=\" \").replace(\"READ MORE\", \"\").strip()\n",
                "                    else:\n",
                "                        review_text = \"\"\n",
                "                        \n",
                "                    # Date (usually in a p tag with class _2sc7ZR)\n",
                "                    # There are usually two of these: name and date\n",
                "                    meta_tags = card.find_all(\"p\", class_=\"_2sc7ZR\")\n",
                "                    if len(meta_tags) >= 2:\n",
                "                        date_str = meta_tags[1].text.strip()\n",
                "                    else:\n",
                "                        date_str = None\n",
                "                        \n",
                "                    if rating and review_text:\n",
                "                        reviews_data.append({\n",
                "                            \"rating\": rating,\n",
                "                            \"review_text\": review_text,\n",
                "                            \"date\": date_str,\n",
                "                            \"source\": \"Flipkart\"\n",
                "                        })\n",
                "                        found_on_page += 1\n",
                "                        \n",
                "                except AttributeError:\n",
                "                    continue\n",
                "            \n",
                "            print(f\"Found {found_on_page} reviews on page {page}. Total: {len(reviews_data)}\")\n",
                "            \n",
                "            if found_on_page == 0:\n",
                "                print(\"No reviews found on this page. Ending scrape.\")\n",
                "                break\n",
                "\n",
                "            page += 1\n",
                "            time.sleep(random.uniform(2, 5)) # Polite delay\n",
                "            \n",
                "        except Exception as e:\n",
                "            print(f\"Error on page {page}: {e}\")\n",
                "            break\n",
                "            \n",
                "    return pd.DataFrame(reviews_data)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d517ba28",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run Scraping\n",
                "if __name__ == \"__main__\":\n",
                "    print(\"Starting scraping...\")\n",
                "    df_reviews = scrape_flipkart_reviews(BASE_URL, target_count=150)\n",
                "    \n",
                "    if not df_reviews.empty:\n",
                "        print(f\"Saved {len(df_reviews)} reviews to {OUTPUT_FILE}\")\n",
                "        df_reviews.to_csv(OUTPUT_FILE, index=False)\n",
                "        print(df_reviews.head())\n",
                "    else:\n",
                "        print(\"No reviews collected. Please check the selectors or URL.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}